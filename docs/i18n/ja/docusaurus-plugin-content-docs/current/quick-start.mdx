---
sidebar_position: 2
---

import Tabs from '@theme/Tabs'
import TabItem from '@theme/TabItem'

# はじめに

:::caution

⚠️ このプロジェクトは現在開発中です。Flappyの初版をできるだけ早くリリースするために、一生懸命に作業しています。お楽しみに！ドキュメンテーションとコード例は近日公開予定です。
:::


Flappyは、LLMベースのエージェント/アプリケーションを構築するためのプログラミング言語に依存しないSDKです。始めるには、使用している言語を指定してください。

## インストール


<Tabs>
  <TabItem value="nodejs" label="NodeJS (TypeScript)" default>

```bash
npm install @plesito/node-flappy@next
# or yarn add @pleisto/node-flappy@next
```

  </TabItem>
  <TabItem value="java" label="Java" default>

Maven

```xml
<dependency>
    <groupId>com.pleisto</groupId>
    <artifactId>flappy</artifactId>
    <version>0.0.7</version>
</dependency>
```

Gradle

```sh
implementation 'com.pleisto:flappy:0.0.7'
```

  </TabItem>
  <TabItem value="kotlin" label="Kotlin" default>

```kotlin
implementation("com.pleisto:flappy:0.0.7")
```

  </TabItem>
  <TabItem value="csharp" label="C#" default>
    Coming soon
  </TabItem>
</Tabs>

## LLMアダプターの作成

Flappyは、OpenAIのChatGPTやhugingfaceのtransformersなど、複数のLLM実装をサポートしています。また、`LLMBase` インターフェースを実装することで、自分自身のLLMアダプターを作成することも可能です。

<Tabs>
  <TabItem value="nodejs-chatgpt" label="NodeJS (TypeScript) with ChatGPT" default>

```ts
// you need to manually install `openai` package.
import { env } from 'node:process'
import OpenAI from 'openai'
import { ChatGPT } from '@pleisto/node-flappy'

const chatGpt = new ChatGPT(
  new OpenAI({
    apiKey: env.OPENAI_API_KEY!,
    baseURL: env.OPENAI_API_BASE!
  }),
  'gpt-3.5-turbo'
)
```

  </TabItem>
  <TabItem value="nodejs-baichuan" label="NodeJS (TypeScript) with Baichuan-53B API" default>

```ts
import { env } from 'node:process'
import { Baichuan } from '@pleisto/node-flappy'

const chatGpt = new Baichuan(
 {
  baichuan_api_key: env.BAICHUAN_API_KEY!,
  baichuan_secret_key: env.BAICHUAN_SECRET_KEY!,
 }
)
```

  </TabItem>
  <TabItem value="java" label="Java" default>

```java
    Dotenv dotenv = Dotenv.load();
    ChatGPT llm = new ChatGPT("gpt-3.5-turbo", new ChatGPT.ChatGPTConfig(dotenv.get("OPENAI_TOKEN"), dotenv.get("OPENAI_API_BASE")));
```

  </TabItem>
  <TabItem value="kotlin" label="Kotlin" default>


```kotlin
  val dotenv = dotenv()

  val chatGPT = ChatGPT(
    model = "gpt-3.5-turbo",
    chatGPTConfig = ChatGPT.ChatGPTConfig(token = dotenv["OPENAI_TOKEN"], host = dotenv["OPENAI_API_BASE"])
  )
```

  </TabItem>
  <TabItem value="csharp" label="C#" default>
    Coming soon
  </TabItem>
</Tabs>

## エージェントの定義

人工知能では、エージェントは環境を認識し、判断を下し、特定の目標または目標セットを達成するための行動を取るように設計されたコンピュータプログラムまたはシステムを指します。エージェントは自律的に動作し、つまり、人間のオペレーターによって直接制御されることはありません。

Flappyのエコシステムでは、エージェントはLLMの多目的な伝送路として作用します。エージェントは、データの構造化、外部APIの呼び出し、またはLLMが生成したPythonコードのサンドボックス化など、様々なタスクを必要に応じて組み合わせて処理するように設計されています。これは硬直した歯車ではなく、効率的で安全なLLMのインタラクションを必要に応じて適応する柔軟なツールです。

### 主要な概念

#### 関数

関数はFlappyフレームワークにおけるエージェントの基盤となります。このドキュメンテーションでは、定義と利用が可能な2つの主要な関数タイプ、`InvokeFunction`と`SythesizedFunction`を紹介します。

- `InvokeFunction`はエージェントが環境と対話するための機能を提供し、LangchainのAgentsのToolsに類似しています。これは入力と出力のパラメータで定義され、その構造はLanguage Learning Model (LLM)が効率的に対話するために明確である必要があります。これらのパラメータと関数がユーザーのリクエストや世界との対話において果たす役割を理解することは、効率的なタスク計画のために重要です。
- `SythesizedFunction`はLLMが演じる関数の一種です。その説明と入力と出力の構造を定義するだけでよく、LLMはその後、定義された形式で入力を処理し、期待される形式で出力を生成しようとします。この特徴は、LLMが構造化データの抽出タスクを実行したり、LLMが構造化データを出力することが期待されるシナリオで、特に有用です。

#### コードインタープリタ

Flappyのコードインタープリタは、ChatGPTコードインタープリタの等価な代替品として機能し、言語モデルがPythonコードを通じて複雑なユーザー要求を満たすことを可能にします。Flappyのコードインタープリタが市場の他のオープンソース実装と異なるのは、そのサンドボックス化された安全機能です。これにより、本番環境での展開に必要な厳格なセキュリティ要件を満たすことが確保されます。


<Tabs>
  <TabItem value="nodejs" label="NodeJS (TypeScript) with ChatGPT" default>

```ts
import { createFlappyAgent,  createInvokeFunction,  createSynthesizedFunction,  z } from '@pleisto/node-flappy'

enum Verdict {
  Innocent = 'Innocent',
  Guilty = 'Guilty',
  Unknown = 'Unknown'
}

const MOCK_LAWSUIT_DATA =
  "As Alex Jones continues telling his Infowars audience about his money problems and pleads for them to buy his products, his own documents show life is not all that bad — his net worth is around $14 million and his personal spending topped $93,000 in July alone, including thousands of dollars on meals and entertainment. The conspiracy theorist and his lawyers file monthly financial reports in his personal bankruptcy case, and the latest one has struck a nerve with the families of victims of Sandy Hook Elementary School shooting. They're still seeking the $1.5 billion they won last year in lawsuits against Jones and his media company for repeatedly calling the 2012 massacre a hoax on his shows. “It is disturbing that Alex Jones continues to spend money on excessive household expenditures and his extravagant lifestyle when that money rightfully belongs to the families he spent years tormenting,” said Christopher Mattei, a Connecticut lawyer for the families. “The families are increasingly concerned and will continue to contest these matters in court.” In an Aug. 29 court filing, lawyers for the families said that if Jones doesn’t reduce his personal expenses to a “reasonable” level, they will ask the bankruptcy judge to bar him from “further waste of estate assets,” appoint a trustee to oversee his spending, or dismiss the bankruptcy case. On his Infowars show Tuesday, Jones said he’s not doing anything wrong."

const agent = createFlappyAgent({
  llm: chatGpt,
  // Define your agent's functions.
  functions: [
    createSynthesizedFunction({
      name: 'getMeta',
      description: 'Extract meta data from a lawsuit full text.',
      args: z.object({
        lawsuit: z.string().describe('Lawsuit full text.')
      }),
      returnType: z.object({
        verdict: z.nativeEnum(Verdict),
        plaintiff: z.string(),
        defendant: z.string(),
        judgeOptions: z.array(z.string())
      })
    }),
    createInvokeFunction({
      name: 'getLatestLawsuitsByPlaintiff',
      description: 'Get the latest lawsuits by plaintiff.',
      args: z.object({
        plaintiff: z.string(),
        arg1: z.boolean().describe('For demo purpose. set to False'),
        arg2: z.array(z.string()).describe('ignore it').optional()
      }),
      returnType: z.string(),
      resolve: async args => {
        // Do something
        // e.g. query SQL database
        console.debug('getLatestLawsuitsByPlaintiff called', args)
        return MOCK_LAWSUIT_DATA
      }
    })
  ],
  // Settings for the code interpreter sandbox
  codeInterpreter: {
    enableNetwork: true,
    env: env as Record<string, string>
  }
})
```

  </TabItem>
  <TabItem value="java" label="Java" default>
    Coming soon
  </TabItem>
  <TabItem value="kotlin" label="Kotlin" default>
    Coming soon
  </TabItem>
  <TabItem value="csharp" label="C#" default>
    Coming soon
  </TabItem>
</Tabs>

## Call your agent

### Create and execute a action plan

Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution.

Flappy uses [ReWOO](https://arxiv.org/abs/2305.18323) instead of [ReAct](https://arxiv.org/abs/2210.03629) to minimize LLM token output, thereby reducing costs. Building on this, the agent is equipped to autonomously devise a plan based on the user's prompt. This involves determining the sequence of functions that need to be invoked to fulfill the given prompt. The execution then proceeds according to this formulated plan, further optimizing the efficiency of our system.

<Tabs>
  <TabItem value="nodejs" label="NodeJS (TypeScript) with ChatGPT" default>

```ts
agent.executePlan('Find the latest case with the plaintiff being families of victims and return its metadata.')
```

  </TabItem>
  <TabItem value="java" label="Java" default>

```java
  Future<LawMetaReturn> future = lawAgent.executePlanAsync(LAW_EXECUTE_PLAN_PROMPT);
```

  </TabItem>
  <TabItem value="kotlin" label="Kotlin" default>

```kotlin
  lawAgent.use {
    it.executePlan<LawMetaReturn>(LAW_EXECUTE_PLAN_PROMPT)
  }
```

  </TabItem>
  <TabItem value="csharp" label="C#" default>
    Coming soon
  </TabItem>
</Tabs>


### Call sythesized function directly

You can also call sythesized function directly without executing a action plan.


<Tabs>
  <TabItem value="nodejs" label="NodeJS (TypeScript) with ChatGPT" default>

```ts
agent.callFunction('getMeta', {lawsuit: MOCK_LAWSUIT_DATA})
```

  </TabItem>
  <TabItem value="java" label="Java" default>
    Coming soon
  </TabItem>
  <TabItem value="kotlin" label="Kotlin" default>
    Coming soon
  </TabItem>
  <TabItem value="csharp" label="C#" default>
    Coming soon
  </TabItem>
</Tabs>



### Call Code Interpreter

Code Interpreter currently needs to be called directly. We are working on a better way to integrate it with the agent.

<Tabs>
  <TabItem value="nodejs" label="NodeJS (TypeScript) with ChatGPT" default>

```ts
agent.callCodeInterpreter(
  'There are some rabbits and chickens in a barn. What is the number of chickens if there are 396 legs  and 150 heads in the barn?'
)
```

  </TabItem>
  <TabItem value="java" label="Java" default>
    Coming soon
  </TabItem>
  <TabItem value="kotlin" label="Kotlin" default>
    Coming soon
  </TabItem>
  <TabItem value="csharp" label="C#" default>
    Coming soon
  </TabItem>
</Tabs>
